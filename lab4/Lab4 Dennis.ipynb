{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solution to deeprlbootcamp lab 4\n",
    "by Dennis Briner\n",
    "\n",
    "### General\n",
    "See main.py, a2c.py, trpo.py, pg.py. I've left the *** YOUR CODE HERE *** so things can be found quick.\n",
    "The code contains explaining comments.\n",
    "\n",
    "## 3 Policy Gradient\n",
    "\n",
    "### 3.7 Hyperparameter Tuning\n",
    "\n",
    "Remarks:\n",
    "* I've tried various options and parameters, I copied the results of some interesting ones.\n",
    "* I've run each run at least 3 times to make sure it wasn't just good/bad luck.\n",
    "\n",
    "#### 3.7.1 Tuning Cart Pole\n",
    "\n",
    "Hypothesis: lower batch size to increase runtime, smaller discount to value more recent actions higher\n",
    "\n",
    "batch_size=100, discount=0.9\n",
    "```\n",
    "Iteration: 95 AverageReturn: 145.00 Entropy: 0.61 Perplexity: 1.84 |theta|_2: 2.53\n",
    "Iteration: 96 AverageReturn: 183.00 Entropy: 0.60 Perplexity: 1.83 |theta|_2: 2.55\n",
    "Iteration: 97 AverageReturn: 200.00 Entropy: 0.61 Perplexity: 1.83 |theta|_2: 2.55\n",
    "Iteration: 98 AverageReturn: 182.00 Entropy: 0.59 Perplexity: 1.81 |theta|_2: 2.55\n",
    "Iteration: 99 AverageReturn: 200.00 Entropy: 0.60 Perplexity: 1.81 |theta|_2: 2.54\n",
    "\n",
    "Finished training in:  0:00:01.378026\n",
    "```\n",
    "FINDING: trains fast and good!\n",
    "\n",
    "batch_size=100, discount=0.99\n",
    "```\n",
    "Iteration: 95 AverageReturn: 78.00 Entropy: 0.55 Perplexity: 1.73 |theta|_2: 2.52\n",
    "Iteration: 96 AverageReturn: 116.00 Entropy: 0.59 Perplexity: 1.81 |theta|_2: 2.52\n",
    "Iteration: 97 AverageReturn: 165.00 Entropy: 0.61 Perplexity: 1.84 |theta|_2: 2.55\n",
    "Iteration: 98 AverageReturn: 67.00 Entropy: 0.59 Perplexity: 1.80 |theta|_2: 2.53\n",
    "Iteration: 99 AverageReturn: 105.00 Entropy: 0.61 Perplexity: 1.85 |theta|_2: 2.55\n",
    "\n",
    "Finished training in: 0:00:01.343715\n",
    "```\n",
    "FINDING: trains fast but worse!\n",
    "\n",
    "batch_size=2000, discount=0.9\n",
    "```\n",
    "Iteration: 95 AverageReturn: 187.18 Entropy: 0.55 Perplexity: 1.74 |theta|_2: 3.87\n",
    "Iteration: 96 AverageReturn: 197.91 Entropy: 0.55 Perplexity: 1.73 |theta|_2: 3.88\n",
    "Iteration: 97 AverageReturn: 196.36 Entropy: 0.56 Perplexity: 1.76 |theta|_2: 3.89\n",
    "Iteration: 98 AverageReturn: 181.82 Entropy: 0.55 Perplexity: 1.74 |theta|_2: 3.87\n",
    "Iteration: 99 AverageReturn: 193.27 Entropy: 0.55 Perplexity: 1.74 |theta|_2: 3.88\n",
    "\n",
    "Finished training in: 0:00:16.125489\n",
    "```\n",
    "FINDING: trains slow but good!\n",
    "\n",
    "--batch_size 100 --discount 0.9 --learning_rate 0.4 --n_itrs 300\n",
    "```\n",
    "Iteration: 295 AverageReturn: 200.00 Entropy: 0.46 Perplexity: 1.59 |theta|_2: 7.85\n",
    "Iteration: 296 AverageReturn: 200.00 Entropy: 0.47 Perplexity: 1.60 |theta|_2: 7.85\n",
    "Iteration: 297 AverageReturn: 200.00 Entropy: 0.45 Perplexity: 1.56 |theta|_2: 7.85\n",
    "Iteration: 298 AverageReturn: 200.00 Entropy: 0.48 Perplexity: 1.61 |theta|_2: 7.85\n",
    "Iteration: 299 AverageReturn: 200.00 Entropy: 0.48 Perplexity: 1.62 |theta|_2: 7.85\n",
    "\n",
    "Finished training in: 0:00:04.929320\n",
    "```\n",
    "FINDING: Lower batch size, lower discount, higher learning rate <- This seems to be the best setting. It reaches the max score of 200.00 around Iteration 65 and stays there constantly after Iteration 180.\n",
    "\n",
    "#### 3.7.2 Tuning Point\n",
    "\n",
    "--batch_size 100 --discount 0.8 --learning_rate 0.4 --n_itrs 100\n",
    "\n",
    "```\n",
    "Iteration: 95 AverageReturn: -20.20 |theta|_2: 7.15\n",
    "Iteration: 96 AverageReturn: -18.63 |theta|_2: 7.40\n",
    "Iteration: 97 AverageReturn: -18.42 |theta|_2: 7.58\n",
    "Iteration: 98 AverageReturn: -17.76 |theta|_2: 7.61\n",
    "Iteration: 99 AverageReturn: -17.98 |theta|_2: 7.45\n",
    "\n",
    "Finished training in: 0:00:00.696435\n",
    "```\n",
    "FINDING: The hyperparameter used to tune the Cart Pole could be moved to the Point environment and improve the result a little bit. Due to the lower batch size, training was much faster too. It turned out, that reducing the discount factor even more, lead to a significantly better result.\n",
    "\n",
    "### 3.8 Natural Gradient\n",
    "\n",
    "```\n",
    "Iteration: 0 AverageReturn: 18.52 Entropy: 0.69 Perplexity: 2.00 |theta|_2: 0.83\n",
    "Iteration: 1 AverageReturn: 23.05 Entropy: 0.68 Perplexity: 1.98 |theta|_2: 1.49\n",
    "Iteration: 2 AverageReturn: 32.08 Entropy: 0.66 Perplexity: 1.93 |theta|_2: 2.66\n",
    "Iteration: 3 AverageReturn: 48.53 Entropy: 0.63 Perplexity: 1.87 |theta|_2: 4.26\n",
    "Iteration: 4 AverageReturn: 69.69 Entropy: 0.60 Perplexity: 1.81 |theta|_2: 6.60\n",
    "Iteration: 5 AverageReturn: 119.41 Entropy: 0.57 Perplexity: 1.76 |theta|_2: 9.21\n",
    "Iteration: 6 AverageReturn: 188.09 Entropy: 0.59 Perplexity: 1.80 |theta|_2: 11.43\n",
    "Iteration: 7 AverageReturn: 169.42 Entropy: 0.56 Perplexity: 1.74 |theta|_2: 11.24\n",
    "Iteration: 8 AverageReturn: 200.00 Entropy: 0.56 Perplexity: 1.74 |theta|_2: 13.79\n",
    "```\n",
    "\n",
    "Reaching 200 already in iteration 8!\n",
    "\n",
    "## 4 Advanced Policy Gradient\n",
    "\n",
    "```\n",
    "[2021-12-21 13:44:21.729586 UTC] Starting iteration 28\n",
    "[2021-12-21 13:44:21.730311 UTC] Start collecting samples\n",
    "100%|██████████████████████████████████████████| 2000/2000 [00:00<00:00, 3527.88it/s]\n",
    "[2021-12-21 13:44:22.304560 UTC] Computing input variables for policy optimization\n",
    "[2021-12-21 13:44:22.329124 UTC] Computing policy gradient\n",
    "[2021-12-21 13:44:22.340398 UTC] Updating baseline\n",
    "[2021-12-21 13:44:22.463318 UTC] Computing logging information\n",
    "------------------------------------\n",
    "| Iteration            | 28        |\n",
    "| SurrLoss             | 0.012269  |\n",
    "| Entropy              | 0.2522    |\n",
    "| Perplexity           | 1.2869    |\n",
    "| AveragePolicyProb[0] | 0.4949    |\n",
    "| AveragePolicyProb[1] | 0.5051    |\n",
    "| AverageReturn        | 200       |\n",
    "| MinReturn            | 200       |\n",
    "| MaxReturn            | 200       |\n",
    "| StdReturn            | 0         |\n",
    "| AverageEpisodeLength | 200       |\n",
    "| MinEpisodeLength     | 200       |\n",
    "| MaxEpisodeLength     | 200       |\n",
    "| StdEpisodeLength     | 0         |\n",
    "| TotalNEpisodes       | 423       |\n",
    "| TotalNSamples        | 56460     |\n",
    "| ExplainedVariance    | -0.058698 |\n",
    "------------------------------------\n",
    "[2021-12-21 13:44:23.046003 UTC] Saving snapshot\n",
    "```\n",
    "\n",
    "Reaching 200 in iteration 28!\n",
    "\n",
    "<img src=\"./image/cartpole-performance.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 5 Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "## 5.3 cartpole\n",
    "\n",
    "```\n",
    "[2021-12-21 14:28:53.740276 UTC] Starting iteration 32\n",
    "[2021-12-21 14:28:53.740939 UTC] Start collecting samples\n",
    "100%|██████████████████████████████████████████| 2000/2000 [00:00<00:00, 4594.84it/s]\n",
    "[2021-12-21 14:28:54.179211 UTC] Computing input variables for policy optimization\n",
    "[2021-12-21 14:28:54.198454 UTC] Performing policy update\n",
    "[2021-12-21 14:28:54.207345 UTC] Computing gradient in Euclidean space\n",
    "[2021-12-21 14:28:54.218187 UTC] Computing approximate natural gradient using conjugate gradient algorithm\n",
    "[2021-12-21 14:28:54.319691 UTC] Performing line search\n",
    "[2021-12-21 14:28:54.341779 UTC] Updating baseline\n",
    "[2021-12-21 14:28:54.442468 UTC] Computing logging information\n",
    "------------------------------------\n",
    "| Iteration            | 32        |\n",
    "| ExpectedImprovement  | 0.015501  |\n",
    "| ActualImprovement    | 0.017922  |\n",
    "| ImprovementRatio     | 1.1562    |\n",
    "| MeanKL               | 0.0084447 |\n",
    "| Entropy              | 0.53379   |\n",
    "| Perplexity           | 1.7054    |\n",
    "| AveragePolicyProb[0] | 0.49449   |\n",
    "| AveragePolicyProb[1] | 0.50551   |\n",
    "| AverageReturn        | 200       |\n",
    "| MinReturn            | 200       |\n",
    "| MaxReturn            | 200       |\n",
    "| StdReturn            | 0         |\n",
    "| AverageEpisodeLength | 200       |\n",
    "| MinEpisodeLength     | 200       |\n",
    "| MaxEpisodeLength     | 200       |\n",
    "| StdEpisodeLength     | 0         |\n",
    "| TotalNEpisodes       | 534       |\n",
    "| TotalNSamples        | 64606     |\n",
    "| ExplainedVariance    | 0.43736   |\n",
    "------------------------------------\n",
    "[2021-12-21 14:28:54.970226 UTC] Saving snapshot\n",
    "```\n",
    "\n",
    "Reaching 200 in iteration 32!\n",
    "\n",
    "## 5.3 pendulum\n",
    "\n",
    "<img src=\"./image/pendulum-performance.png\" width=\"400\">\n",
    "\n",
    "\n",
    "## 5.3 half cheetah\n",
    "\n",
    "Iteration 1 (just falling):\n",
    "<img src=\"./image/half_cheetah_iteration_1.gif\" width=\"200\">\n",
    "\n",
    "Iteration 50 (falling forward):\n",
    "<img src=\"./image/half_cheetah_iteration_50.gif\" width=\"200\">\n",
    "\n",
    "Iteration 230 (not falling, but also not moving):\n",
    "<img src=\"./image/half_cheetah_iteration_230.gif\" width=\"200\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 (Synchronous) Advantage Actor-Critic (A2C)\n",
    "\n",
    "## 6.3 Pong\n",
    "\n",
    "Iteration 20 (starts winning against NPC):\n",
    "<img src=\"./image/pong_iteration_20.gif\" width=\"200\">\n",
    "\n",
    "Iteration 36 (makes every point almost instantly):\n",
    "<img src=\"./image/pong_iteration_36.gif\" width=\"200\">\n",
    "\n",
    "Converges after roughly 30 training iterations:\n",
    "<img src=\"./image/pong-performance.png\" width=\"400\">\n",
    "\n",
    "## 6.3 Breakout\n",
    "\n",
    "Iteration 4 (has no clue what it's doing and by change luckily gets a point):\n",
    "<img src=\"./image/breakout_iteration_4.gif\" width=\"200\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}