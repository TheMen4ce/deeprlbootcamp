{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Solution to deeprlbootcamp lab 3 by Dennis Briner\n",
    "\n",
    "# DQN\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aka: (s, a, r, s', d)\n",
    "def compute_q_learning_loss(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "    \"\"\"\n",
    "    :param l_obs: A chainer variable holding a list of observations. Should be of shape N * |S|.\n",
    "    :param l_act: A chainer variable holding a list of actions. Should be of shape N.\n",
    "    :param l_rew: A chainer variable holding a list of rewards. Should be of shape N.\n",
    "    :param l_next_obs: A chainer variable holding a list of observations at the next time step. Should be of\n",
    "    shape N * |S|.\n",
    "    :param l_done: A chainer variable holding a list of binary values (indicating whether episode ended after this\n",
    "    time step). Should be of shape N.\n",
    "    :return: A chainer variable holding a scalar loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute Q(s,a)\n",
    "    q_value = F.select_item(self._q.forward(l_obs), l_act)\n",
    "\n",
    "    # compute y = r + y max(Q(s',a')) or y = r respectively\n",
    "    # (1 - l_done) will be 0 if considering a terminal state => y = r\n",
    "    y = l_rew + (1 - l_done) * self._discount * F.max(self._qt.forward(l_next_obs), axis=1)\n",
    "\n",
    "    # compute loss using MSE (y - Q(s,a))^2\n",
    "    loss = F.mean_squared_error(y, q_value)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "We see fast improvements within the first ten iterations. Afterwards it doesn't improve drastically since we have already reached an optimum solution.\n",
    "\n",
    "<img src=\"dqn-grid.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DDQN\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aka: (s, a, r, s', d)\n",
    "def compute_double_q_learning_loss(self, l_obs, l_act, l_rew, l_next_obs, l_done):\n",
    "    \"\"\"\n",
    "    :param l_obs: A chainer variable holding a list of observations. Should be of shape N * |S|.\n",
    "    :param l_act: A chainer variable holding a list of actions. Should be of shape N.\n",
    "    :param l_rew: A chainer variable holding a list of rewards. Should be of shape N.\n",
    "    :param l_next_obs: A chainer variable holding a list of observations at the next time step. Should be of\n",
    "    shape N * |S|.\n",
    "    :param l_done: A chainer variable holding a list of binary values (indicating whether episode ended after this\n",
    "    time step). Should be of shape N.\n",
    "    :return: A chainer variable holding a scalar loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute Q(s,a)\n",
    "    q_value = F.select_item(self._q.forward(l_obs), l_act)\n",
    "\n",
    "    # compute arg max(Q(s'))\n",
    "    a_prime = F.argmax(self._q.forward(l_next_obs), axis=1)\n",
    "\n",
    "    # compute Q(s', arg max(Q(s')) )\n",
    "    double_q = F.select_item(self._qt.forward(l_next_obs), a_prime)\n",
    "\n",
    "    # compute y = r + Q(s', arg max(Q(s')) ) or y = r respectively\n",
    "    # (1 - l_done) will be 0 if considering a terminal state => y = r\n",
    "    y = l_rew + (1 - l_done) * self._discount * double_q\n",
    "\n",
    "    # compute loss using MSE (y - Q(s,a))^2\n",
    "    loss = F.mean_squared_error(y, q_value)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results Grid World\n",
    "\n",
    "We don't see much of a difference compared to the DQN version. At least it's not getting worse. The improvements should be mainly in terms of performance. Performance may be neglected in the simple grid world environment.\n",
    "\n",
    "<img src=\"ddqn-grid.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results Pong\n",
    "\n",
    "This is the result I've gotten after training for 200+ iterations. It seems to improve, but very slowly. When playing the game, the agent still looses.\n",
    "\n",
    "<img src=\"ddqn-pong.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modifying hyperparameters\n",
    "\n",
    "I went on and tried to modify the hyperparameters. My hypothesis was, that the exploration rate was too high, since the reward always dropped pretty hard after a few iterations. My second hypothesis was, that I should decrease the discount factor. In pong only the last few actions are relevant for loosing a point and I wanted to value this more.\n",
    "\n",
    "This is how I've adjusted the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-900c97629259>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Training procedure length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0minitial_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minitial_step\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mmax_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_steps\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mlearning_start_itr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_steps\u001B[0m \u001B[0;34m//\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'initial_step' is not defined"
     ]
    }
   ],
   "source": [
    "# DQN gamma parameter\n",
    "discount=0.9,\n",
    "\n",
    "# Training procedure length\n",
    "initial_step=initial_step,\n",
    "max_steps=max_steps,\n",
    "learning_start_itr=max_steps // 100,\n",
    "# Frequency of copying the actual Q to the target Q\n",
    "target_q_update_freq=target_q_update_freq,\n",
    "# Frequency of updating the Q-value function\n",
    "train_q_freq=4,\n",
    "# Double Q\n",
    "double_q=double,\n",
    "\n",
    "# Exploration parameters\n",
    "initial_eps=1.0,\n",
    "final_eps=0.01,\n",
    "fraction_eps=0.01,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sadly there wasn't much improvement visible. I still went further on playing with the hyperparameters, but I couldn't find any combination that would make the agent learn very fast and good. The biggest issue here was training time on my machine (2.6 GHz 6-Core Intel Core i7). Just training a 100 iterations took over 1 hour. This kept me from experimenting more.\n",
    "\n",
    "<img src=\"ddqn-pong-hyper.png\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When observing the agent play, I could see that it has figured out that it needs to move the paddle towards the ball. But it's not moving close enough. Maybe this could be due to my lowered discount factor:\n",
    "\n",
    "<img src=\"pong-error.gif\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}